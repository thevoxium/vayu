<!DOCTYPE html>
<html lang="en">
<head>
   <meta charset="UTF-8">
   <meta name="viewport" content="width=device-width, initial-scale=1.0">
   <title>Vayu - README</title>
   <link href="https://api.fontshare.com/v2/css?f[]=satoshi@400,500,700&display=swap" rel="stylesheet">
   <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" rel="stylesheet">
   <style>
       * {
           margin: 0;
           padding: 0;
           box-sizing: border-box;
       }
       :root {
           --bg-primary: #0a0a0b;
           --bg-secondary: #111113;
           --text-primary: #ffffff;
           --text-secondary: #a3a3a3;
           --text-muted: #6b7280;
           --border: #1f1f23;
           --accent: #3b82f6;
           --accent-red: #ef4444;
           --code-bg: #1a1a1a;
       }
       body {
           font-family: 'Satoshi', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
           background-color: var(--bg-primary);
           color: var(--text-primary);
           line-height: 1.6;
           letter-spacing: -0.04em;
       }
       .container {
           max-width: 800px;
           margin: 0 auto;
           padding: 4rem 2rem;
       }
       .back-link {
           display: inline-block;
           color: white;
           text-decoration: underline;
           text-decoration-color: var(--accent-red);
           font-size: 1.1rem;
           font-weight: 700;
           margin-bottom: 3rem;
           letter-spacing: -0.04em;
           transition: opacity 0.2s ease;
       }
       .back-link:hover {
           opacity: 0.8;
       }
       h1 {
           font-size: clamp(3rem, 6vw, 4rem);
           font-weight: 700;
           margin-bottom: 1rem;
           color: var(--text-primary);
           letter-spacing: -0.04em;
       }
       h1 .dot {
           color: var(--accent-red);
       }
       .tagline {
           font-size: 1.25rem;
           color: var(--text-secondary);
           font-weight: 700;
           margin-bottom: 2rem;
           letter-spacing: -0.04em;
       }
       .github-badge {
           margin-bottom: 2rem;
       }
       .github-badge img {
           height: 20px;
       }
       h2 {
           font-size: 1.5rem;
           font-weight: 700;
           color: var(--text-primary);
           margin: 2.5rem 0 1rem 0;
           letter-spacing: -0.04em;
       }
       h3 {
           font-size: 1.25rem;
           font-weight: 700;
           color: var(--text-primary);
           margin: 2rem 0 0.75rem 0;
           letter-spacing: -0.04em;
       }
       h4 {
           font-size: 1.1rem;
           font-weight: 700;
           color: var(--text-primary);
           margin: 1.5rem 0 0.5rem 0;
           letter-spacing: -0.04em;
       }
       p {
           color: var(--text-secondary);
           margin-bottom: 1rem;
       }
       ul {
           color: var(--text-secondary);
           margin-bottom: 1rem;
           padding-left: 1.5rem;
       }
       li {
           margin-bottom: 0.5rem;
       }
       pre {
           background-color: var(--code-bg);
           border: 1px solid var(--border);
           border-radius: 8px;
           padding: 1.5rem;
           margin: 1rem 0;
           overflow-x: auto;
           font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
           font-size: 0.9rem;
           line-height: 1.4;
       }
       code {
           background-color: var(--code-bg);
           padding: 0.2rem 0.4rem;
           border-radius: 4px;
           font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
           font-size: 0.9em;
           color: #e6e6e6;
       }
       pre code {
           background: none;
           padding: 0;
           border-radius: 0;
       }
       .cpp {
           color: #9cdcfe;
       }
       .bash {
           color: #98c379;
       }
       .output {
           color: #d19a66;
       }
       strong {
           color: var(--text-primary);
           font-weight: 700;
       }
       @media (max-width: 768px) {
           .container {
               padding: 2rem 1rem;
           }
           pre {
               padding: 1rem;
               font-size: 0.85rem;
           }
       }
   </style>
</head>
<body>
   <div class="container">
       <a href="index.html" class="back-link">← back to home</a>
       
       <h1>vayu<span class="dot">.</span></h1>
       <p class="tagline">A lightweight deep learning framework built from scratch in C++</p>
       
       <div class="github-badge">
           <a href="https://github.com/thevoxium/vayu-ml">
               <img src="https://img.shields.io/github/stars/thevoxium/vayu-ml?style=social" alt="GitHub stars">
           </a>
       </div>

       <h2>Features</h2>
       <ul>
           <li><strong>Automatic Differentiation</strong>: Full backward pass support for scalars and tensors</li>
           <li><strong>Broadcasting</strong>: NumPy-style tensor broadcasting for flexible operations</li>
           <li><strong>BLAS Optimization</strong>: Accelerated matrix operations using OpenBLAS, MKL, or Apple Accelerate</li>
           <li><strong>Modern C++</strong>: Built with C++17 features and smart pointers</li>
           <li><strong>Lightweight</strong>: Minimal dependencies, easy to integrate and understand</li>
       </ul>

       <h2>Quick Start</h2>
       
       <h3>Tensor Operations with Broadcasting</h3>
       <pre><code class="cpp">#include "include/tensor.h"
#include &lt;iostream&gt;

int main() {
    auto a = random_tensor({2, 2}, true);     // 2x2 random tensor
    auto b = make_ones({2, 1}, true);         // 2x1 ones tensor
    
    auto c = a + b;                           // Broadcasting addition
    auto d = a * c;                           // Element-wise multiplication
    
    d-&gt;backward();                            // Compute gradients
    
    // Access gradients
    for (auto grad : a-&gt;grad) {
        std::cout &lt;&lt; grad &lt;&lt; ", ";
    }
    
    return 0;
}</code></pre>

       <h3>Linear Regression Example</h3>
       <pre><code class="cpp">#include "include/tensor.h"
#include &lt;iostream&gt;

int main() {
    // Training data: y = 2x + 1
    auto X = tensor({1.0f, 2.0f, 3.0f, 4.0f, 5.0f}, {5, 1}, false);
    auto y = tensor({3.0f, 5.0f, 7.0f, 9.0f, 11.0f}, {5, 1}, false);
    
    // Model parameters
    auto W = tensor({0.1f}, {1, 1}, true);
    auto b = tensor({0.1f}, {1, 1}, true);
    
    float learning_rate = 0.01f;
    int epochs = 200;
    
    for (int epoch = 0; epoch &lt; epochs; epoch++) {
        // Zero gradients
        W-&gt;zero_grad();
        b-&gt;zero_grad();
        
        // Forward pass
        auto y_pred = X-&gt;mm(W) + b;
        auto diff = y_pred - y;
        auto squared_error = diff * diff;
        auto loss = squared_error-&gt;sum() * tensor({1.0f / y-&gt;numel()}, {1, 1}, false);
        
        // Backward pass
        loss-&gt;backward();
        
        // Update parameters
        W-&gt;data[0] -= learning_rate * W-&gt;grad[0];
        b-&gt;data[0] -= learning_rate * b-&gt;grad[0];
        
        if (epoch % 20 == 0) {
            std::cout &lt;&lt; "Epoch " &lt;&lt; epoch &lt;&lt; " | Loss: " &lt;&lt; loss-&gt;data[0] 
                      &lt;&lt; " | W: " &lt;&lt; W-&gt;data[0] &lt;&lt; " | b: " &lt;&lt; b-&gt;data[0] &lt;&lt; std::endl;
        }
    }
    
    return 0;
}</code></pre>

       <h2>Building</h2>
       
       <h3>Requirements</h3>
       <ul>
           <li>C++17 compatible compiler (GCC 7+, Clang 5+, MSVC 2017+)</li>
           <li>Optional: OpenBLAS, Intel MKL, or Apple Accelerate for optimized matrix operations</li>
       </ul>

       <h3>Basic Build</h3>
       <pre><code class="bash">git clone https://github.com/thevoxium/vayu-ml.git
cd vayu-ml
g++ -std=c++17 -O2 -o grad examples/main.cpp src/*.cpp && ./grad</code></pre>

       <h3>With OpenBLAS Optimization (Recommended)</h3>
       
       <h4>macOS (Homebrew):</h4>
       <pre><code class="bash"># Install OpenBLAS
brew install openblas

# Compile with OpenBLAS
g++ -std=c++17 -O3 -DUSE_OPENBLAS -march=native \
    -I/opt/homebrew/opt/openblas/include \
    -L/opt/homebrew/opt/openblas/lib \
    examples/test.cpp src/tensor.cpp -lopenblas -o grad && ./grad</code></pre>

       <h4>Ubuntu/Debian:</h4>
       <pre><code class="bash"># Install OpenBLAS
sudo apt-get install libopenblas-dev

# Compile with OpenBLAS
g++ -std=c++17 -O3 -DUSE_OPENBLAS -march=native \
    -I/usr/include/openblas \
    examples/test.cpp src/tensor.cpp -lopenblas -o grad && ./grad</code></pre>

       <h3>With Apple Accelerate Framework (macOS)</h3>
       <pre><code class="bash">g++ -std=c++17 -O3 -march=native -framework Accelerate \
    examples/test.cpp src/tensor.cpp -o grad && ./grad</code></pre>

       <h3>With Intel MKL</h3>
       <pre><code class="bash">g++ -std=c++17 -O3 -DUSE_MKL -march=native \
    -I${MKLROOT}/include \
    -L${MKLROOT}/lib/intel64 \
    examples/test.cpp src/tensor.cpp -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -o grad && ./grad</code></pre>

       <h2>API Reference</h2>
       
       <h3>Tensor Class</h3>
       
       <h4>Constructor</h4>
       <pre><code class="cpp">Tensor(const std::vector&lt;float&gt;& data, const std::vector&lt;size_t&gt;& shape, bool requires_grad = true)
Tensor(const std::vector&lt;size_t&gt;& shape, bool requires_grad = true)</code></pre>

       <h4>Factory Functions</h4>
       <pre><code class="cpp">// Create tensor from data
auto t = tensor({1.0f, 2.0f, 3.0f, 4.0f}, {2, 2}, true);

// Create tensor with specific shape (initialized with zeros)
auto t = tensor({3, 3}, true);

// Create random tensor
auto t = random_tensor({2, 3}, true, 0.0f, 1.0f);  // min=0.0, max=1.0

// Create tensor filled with ones
auto t = make_ones({2, 3}, true);</code></pre>

       <h4>Basic Operations</h4>
       <pre><code class="cpp">// Element-wise operations (with broadcasting)
auto c = a + b;    // Addition
auto c = a - b;    // Subtraction  
auto c = a * b;    // Element-wise multiplication

// Matrix multiplication
auto c = a-&gt;mm(b);              // Standard matrix multiplication
auto c = a-&gt;mm(b, true);        // BLAS-optimized (default)
auto c = a-&gt;mm(b, false);       // Non-optimized implementation</code></pre>

       <h4>Activation Functions</h4>
       <pre><code class="cpp">auto activated = tensor-&gt;relu();     // ReLU activation
auto activated = tensor-&gt;sigmoid();  // Sigmoid activation</code></pre>

       <h4>Tensor Manipulation</h4>
       <pre><code class="cpp">auto summed = tensor-&gt;sum();                    // Sum all elements
auto transposed = tensor-&gt;transpose();          // Matrix transpose
auto reshaped = tensor-&gt;reshape({4, 2});       // Reshape tensor</code></pre>

       <h4>Gradient Operations</h4>
       <pre><code class="cpp">tensor-&gt;backward();     // Compute gradients via backpropagation
tensor-&gt;zero_grad();    // Zero out gradients
tensor-&gt;clear_graph();  // Clear computation graph</code></pre>

       <h4>Properties</h4>
       <pre><code class="cpp">size_t count = tensor-&gt;numel();           // Number of elements
std::vector&lt;size_t&gt; dims = tensor-&gt;shape; // Tensor dimensions
std::vector&lt;float&gt; values = tensor-&gt;data; // Raw data access
std::vector&lt;float&gt; grads = tensor-&gt;grad;  // Gradient values
bool trainable = tensor-&gt;requires_grad;   // Gradient tracking flag</code></pre>

       <h3>Broadcasting Rules</h3>
       <p>Vayu follows NumPy-style broadcasting rules:</p>
       <ol>
           <li><strong>Trailing dimensions alignment</strong>: Dimensions are aligned from the rightmost</li>
           <li><strong>Size compatibility</strong>: Dimensions are compatible if they are equal, or one of them is 1</li>
           <li><strong>Missing dimensions</strong>: Missing dimensions are treated as size 1</li>
       </ol>

       <p>Examples:</p>
       <pre><code class="cpp">// Compatible shapes
(2, 3) + (1, 3) → (2, 3)    // Broadcast first dimension
(4, 1) * (4, 5) → (4, 5)    // Broadcast second dimension  
(3, 1, 2) + (1, 4, 1) → (3, 4, 2)  // Broadcast multiple dimensions

// Check compatibility
bool compatible = Tensor::can_broadcast({2, 3}, {1, 3});  // true
auto result_shape = Tensor::broadcast_shape({2, 3}, {1, 3});  // {2, 3}</code></pre>

       <h2>Performance Tips</h2>
       <ol>
           <li><strong>Use BLAS optimization</strong>: Always compile with <code>-DUSE_OPENBLAS</code> or equivalent for matrix operations</li>
           <li><strong>Enable compiler optimizations</strong>: Use <code>-O3 -march=native</code> for best performance</li>
           <li><strong>Batch operations</strong>: Perform operations on larger tensors when possible</li>
           <li><strong>Memory management</strong>: Use <code>clear_graph()</code> to free computation graphs when not needed</li>
           <li><strong>Gradient accumulation</strong>: Call <code>zero_grad()</code> before each backward pass in training loops</li>
       </ol>

       <h2>Examples</h2>
       <p>See the <code>examples/</code> directory for more complete examples:</p>
       <ul>
           <li><code>examples/test.cpp</code> - Basic tensor operations and broadcasting</li>
           <li><code>examples/linear_regression_test.cpp</code> - Linear regression implementation</li>
           <li><code>examples/blas_mm.cpp</code> - Matrix multiplication benchmarks</li>
       </ul>

       <h2>License</h2>
       <p>MIT License</p>
   </div>
</body>
</html>
